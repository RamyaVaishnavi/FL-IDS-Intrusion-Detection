{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05e56251-2a56-4bae-b9ce-3f40d61b917d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 data size: 79043 samples\n",
      "Client 2 data size: 63234 samples\n",
      "Client 3 data size: 50587 samples\n",
      "Client 4 data size: 40469 samples\n",
      "Client 5 data size: 32375 samples\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the KDD99 dataset (Assuming you have the dataset file 'kddcup.data_10_percent_corrected')\n",
    "df = pd.read_csv('kddcup.data_10_percent_corrected', header=None)\n",
    "\n",
    "# Define the column names based on KDD99 dataset features\n",
    "columns = [\n",
    "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', \n",
    "    'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', \n",
    "    'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', \n",
    "    'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', \n",
    "    'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', \n",
    "    'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', \n",
    "    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', \n",
    "    'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', \n",
    "    'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'label'\n",
    "]\n",
    "df.columns = columns\n",
    "\n",
    "# Step 1: Encode categorical features\n",
    "for col in ['protocol_type', 'service', 'flag']:\n",
    "    df[col] = LabelEncoder().fit_transform(df[col])\n",
    "\n",
    "# Step 2: Encode the label (binary classification: normal vs attack)\n",
    "df['label'] = df['label'].apply(lambda x: 1 if x != 'normal.' else 0)\n",
    "\n",
    "# Step 3: Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "df[df.columns[:-1]] = scaler.fit_transform(df[df.columns[:-1]])\n",
    "\n",
    "# Step 4: Split the dataset into features and labels\n",
    "X = df.drop('label', axis=1).values\n",
    "y = df['label'].values\n",
    "\n",
    "# Step 5: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to tensor\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create TensorDataset and DataLoader for IID setup (optional, use for evaluation)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Step 6: Simulating Non-IID Data Split for Clients\n",
    "def split_noniid_data(X, y, num_clients):\n",
    "    \"\"\"\n",
    "    Split data in a non-IID fashion among clients.\n",
    "    Each client gets data with biased label distributions.\n",
    "    \"\"\"\n",
    "    non_iid_data = []\n",
    "    unique_labels = np.unique(y)\n",
    "    \n",
    "    # Split data by labels\n",
    "    label_indices = {label: np.where(y == label)[0] for label in unique_labels}\n",
    "    \n",
    "    for client_id in range(num_clients):\n",
    "        client_data_indices = []\n",
    "        for label in unique_labels:\n",
    "            # Each client gets a portion of data for each label (biased distribution)\n",
    "            num_samples = int(len(label_indices[label]) / num_clients)\n",
    "            selected_indices = np.random.choice(label_indices[label], num_samples, replace=False)\n",
    "            client_data_indices.extend(selected_indices)\n",
    "            \n",
    "            # Remove selected indices to avoid overlap between clients\n",
    "            label_indices[label] = np.setdiff1d(label_indices[label], selected_indices)\n",
    "        \n",
    "        # Add the client's data to the list\n",
    "        client_data_X = X[client_data_indices]\n",
    "        client_data_y = y[client_data_indices]\n",
    "        non_iid_data.append((client_data_X, client_data_y))\n",
    "    \n",
    "    return non_iid_data\n",
    "\n",
    "# Simulate 5 clients with non-IID data\n",
    "num_clients = 5\n",
    "client_data_splits = split_noniid_data(X_train, y_train, num_clients)\n",
    "\n",
    "# Example of how to convert each client's data to PyTorch tensors\n",
    "client_datasets = []\n",
    "for client_data_X, client_data_y in client_data_splits:\n",
    "    client_X_tensor = torch.tensor(client_data_X, dtype=torch.float32)\n",
    "    client_y_tensor = torch.tensor(client_data_y, dtype=torch.long)\n",
    "    client_datasets.append(TensorDataset(client_X_tensor, client_y_tensor))\n",
    "\n",
    "# Print client data sizes for verification\n",
    "for i, dataset in enumerate(client_datasets):\n",
    "    print(f\"Client {i+1} data size: {len(dataset)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3abd6b28-fb6e-4cd3-ad87-d73006c08546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# CNN model for sequential data\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "\n",
    "        # Calculate the flattened size after conv/pooling layers\n",
    "        conv_output_size = input_size // 4  # After 2 pooling layers, input size is reduced by a factor of 4\n",
    "        self.fc1 = nn.Linear(32 * conv_output_size, 128)\n",
    "        self.fc2 = nn.Linear(128, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension for Conv1d\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68e1ecc8-ef71-4f7d-a3f0-418f76c14ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_attack(model, data, target, epsilon=0.1): #999\n",
    "    data.requires_grad = True\n",
    "    output = model(data)\n",
    "    loss = nn.CrossEntropyLoss()(output, target)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    perturbed_data = data + epsilon * data.grad.sign()\n",
    "    return perturbed_data.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2a6c181-d032-427e-a0e8-31721f8ce679",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, client_id, model, dataset, lr=0.001, mu=0.1, epsilon=0.2, delta=1e-5):\n",
    "        self.client_id = client_id\n",
    "        self.model = copy.deepcopy(model)\n",
    "        self.dataset = dataset\n",
    "        self.dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.mu = mu\n",
    "        self.epsilon = epsilon\n",
    "        self.delta = delta\n",
    "        self.privacy_engine = PrivacyEngine()\n",
    "        self.model, self.optimizer, self.dataloader = self.privacy_engine.make_private(\n",
    "            module=self.model,\n",
    "            optimizer=self.optimizer,\n",
    "            data_loader=self.dataloader,\n",
    "            noise_multiplier=0.3,\n",
    "            max_grad_norm=1.5\n",
    "        )\n",
    "    \n",
    "    def train_local(self, global_model, epochs=1, adv_training=True, fkd=True):\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            for data, target in self.dataloader:\n",
    "                data, target = data.to(torch.float32), target.to(torch.long)\n",
    "                if adv_training:\n",
    "                    data = adversarial_attack(self.model, data, target)\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, target)\n",
    "                prox_loss = sum(torch.norm(param - global_param) ** 2 for param, global_param in zip(self.model.parameters(), global_model.parameters()))\n",
    "                loss += (self.mu / 2) * prox_loss\n",
    "                if fkd:\n",
    "                    with torch.no_grad():\n",
    "                        global_output = global_model(data)\n",
    "                    distillation_loss = nn.KLDivLoss(reduction='batchmean')(F.log_softmax(output, dim=1), F.softmax(global_output, dim=1))\n",
    "                    loss += 0.4 * distillation_loss\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "        return self.model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "688a9c37-8287-4c4b-9201-e1c9990b63e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "class Server:\n",
    "    def __init__(self, model, num_clients):\n",
    "        self.global_model = model\n",
    "        self.num_clients = num_clients\n",
    "        self.clients = []\n",
    "\n",
    "    def register_client(self, client):\n",
    "        self.clients.append(client)\n",
    "\n",
    "    def aggregate_weights(self, client_weights):\n",
    "        avg_weights = copy.deepcopy(client_weights[0])\n",
    "        for key in avg_weights.keys():\n",
    "            avg_weights[key] = torch.stack([client_weight[key] for client_weight in client_weights]).mean(dim=0)\n",
    "\n",
    "        # Clean weight keys by removing any \"_module.\" prefix\n",
    "        clean_weights = {key.replace(\"_module.\", \"\"): value for key, value in avg_weights.items()}\n",
    "        return clean_weights\n",
    "\n",
    "    def federated_training(self, rounds=10, epochs=1, adv_training=True, fkd=True, dynamic_fed=True):\n",
    "        for r in range(rounds):\n",
    "            # Select clients for the current round\n",
    "            selected_clients = self.clients if not dynamic_fed else np.random.choice(self.clients, max(1, len(self.clients) // 2), replace=False)\n",
    "            selected_clients = list(selected_clients)\n",
    "\n",
    "            # Collect local weights from selected clients\n",
    "            client_weights = [client.train_local(self.global_model, epochs, adv_training, fkd) for client in selected_clients]\n",
    "\n",
    "            # Aggregate and update global model weights\n",
    "            aggregated_weights = self.aggregate_weights(client_weights)\n",
    "            self.global_model.load_state_dict(aggregated_weights)\n",
    "\n",
    "            print(f'Round {r + 1} completed.')\n",
    "\n",
    "    def evaluate_model(self, test_loader):\n",
    "        self.global_model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "        total_loss = 0.0\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(torch.float32), target.to(torch.long)\n",
    "                output = self.global_model(data)\n",
    "                loss = criterion(output, target)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                predictions = torch.argmax(output, dim=1)\n",
    "                y_true.extend(target.numpy())\n",
    "                y_pred.extend(predictions.numpy())\n",
    "\n",
    "        # Compute evaluation metrics\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred, average='macro')\n",
    "        recall = recall_score(y_true, y_pred, average='macro')\n",
    "        f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "        print(\"Evaluation Metrics:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "\n",
    "        return accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1086f4fc-1425-4b84-882b-dd591d6724a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
