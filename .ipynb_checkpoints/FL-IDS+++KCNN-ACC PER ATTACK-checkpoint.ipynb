{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab2cec04-cf49-4444-96cb-79f74bbfb056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 data size: 32923 samples\n",
      "Client 2 data size: 30181 samples\n",
      "Client 3 data size: 27665 samples\n",
      "Client 4 data size: 25362 samples\n",
      "Client 5 data size: 23246 samples\n",
      "Client 6 data size: 21310 samples\n",
      "Client 7 data size: 19532 samples\n",
      "Client 8 data size: 17905 samples\n",
      "Client 9 data size: 16414 samples\n",
      "Client 10 data size: 15043 samples\n",
      "Client 11 data size: 13791 samples\n",
      "Client 12 data size: 12641 samples\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from opacus import PrivacyEngine\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load the KDD99 dataset (Assuming you have the dataset file 'kddcup.data_10_percent_corrected')\n",
    "df = pd.read_csv('kddcup.data_10_percent_corrected', header=None)\n",
    "\n",
    "# Define the column names based on KDD99 dataset features\n",
    "columns = [\n",
    "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', \n",
    "    'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', \n",
    "    'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', \n",
    "    'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', \n",
    "    'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', \n",
    "    'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', \n",
    "    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', \n",
    "    'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', \n",
    "    'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'label'\n",
    "]\n",
    "df.columns = columns\n",
    "\n",
    "# Step 1: Encode categorical features\n",
    "for col in ['protocol_type', 'service', 'flag']:\n",
    "    df[col] = LabelEncoder().fit_transform(df[col])\n",
    "\n",
    "for col in ['protocol_type', 'service', 'flag']:\n",
    "    df[col] = LabelEncoder().fit_transform(df[col])\n",
    "\n",
    "# Step 2: Save original attack type labels\n",
    "df['attack_type'] = df['label']  # Save string labels for later reporting\n",
    "\n",
    "# Step 3: Encode the attack labels (multi-class classification)\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['label']) \n",
    "\n",
    "\n",
    "# Step 3: Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "df[df.columns[:-1]] = scaler.fit_transform(df[df.columns[:-1]])\n",
    "\n",
    "# Step 4: Split the dataset into features and labels\n",
    "X = df.drop(['label', 'attack_type'], axis=1).values\n",
    "y = df['label'].values\n",
    "attack_names = label_encoder.classes_\n",
    "\n",
    "# Step 5: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to tensor\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create TensorDataset and DataLoader for IID setup (optional, use for evaluation)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Step 6: Simulating Non-IID Data Split for Clients\n",
    "def split_noniid_data(X, y, num_clients):\n",
    "    \"\"\"\n",
    "    Split data in a non-IID fashion among clients.\n",
    "    Each client gets data with biased label distributions.\n",
    "    \"\"\"\n",
    "    non_iid_data = []\n",
    "    unique_labels = np.unique(y)\n",
    "    \n",
    "    # Split data by labels\n",
    "    label_indices = {label: np.where(y == label)[0] for label in unique_labels}\n",
    "    \n",
    "    for client_id in range(num_clients):\n",
    "        client_data_indices = []\n",
    "        for label in unique_labels:\n",
    "            # Each client gets a portion of data for each label (biased distribution)\n",
    "            num_samples = int(len(label_indices[label]) / num_clients)\n",
    "            selected_indices = np.random.choice(label_indices[label], num_samples, replace=False)\n",
    "            client_data_indices.extend(selected_indices)\n",
    "            \n",
    "            # Remove selected indices to avoid overlap between clients\n",
    "            label_indices[label] = np.setdiff1d(label_indices[label], selected_indices)\n",
    "        \n",
    "        # Add the client's data to the list\n",
    "        client_data_X = X[client_data_indices]\n",
    "        client_data_y = y[client_data_indices]\n",
    "        non_iid_data.append((client_data_X, client_data_y))\n",
    "    \n",
    "    return non_iid_data\n",
    "\n",
    "num_clients = 12\n",
    "client_data_splits = split_noniid_data(X_train, y_train, num_clients)\n",
    "\n",
    "# Example of how to convert each client's data to PyTorch tensors\n",
    "client_datasets = []\n",
    "for client_data_X, client_data_y in client_data_splits:\n",
    "    client_X_tensor = torch.tensor(client_data_X, dtype=torch.float32)\n",
    "    client_y_tensor = torch.tensor(client_data_y, dtype=torch.long)\n",
    "    client_datasets.append(TensorDataset(client_X_tensor, client_y_tensor))\n",
    "\n",
    "\n",
    "# Print client data sizes for verification\n",
    "for i, dataset in enumerate(client_datasets):\n",
    "    print(f\"Client {i+1} data size: {len(dataset)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4035391e-97b6-43a6-9dd6-e5d0774f7760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import copy\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from opacus import PrivacyEngine  # Differential Privacy\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a06b4f7f-7ef0-4956-af68-85fa9eb57cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_channels=1, output_size=2, input_length=41):  # Corrected __init__\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Calculate the flattened size after convolution\n",
    "        self.fc_input_dim = 32 * input_length  # Adjust if pooling is added\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.fc_input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.ndim == 2:  # If shape is [batch_size, features]\n",
    "            x = x.unsqueeze(1)  # Add a channel dimension: [batch_size, 1, features]\n",
    "        elif x.ndim == 4:  # If shape is [batch_size, 1, 1, features]\n",
    "            x = x.squeeze(2)  # Remove the extra dimension: [batch_size, 1, features]\n",
    "\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a572fcc-90b5-4b0b-bb7d-b522d41ce1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_attack(model, data, target, epsilon=0.1): #999\n",
    "    data.requires_grad = True\n",
    "    output = model(data)\n",
    "    loss = nn.CrossEntropyLoss()(output, target)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    perturbed_data = data + epsilon * data.grad.sign()\n",
    "    return perturbed_data.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65b1561b-64aa-493b-9e36-ae6aa1c30b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, client_id, model, dataset, lr=0.001, mu=0.1, epsilon=0.2, delta=1e-5):\n",
    "        self.client_id = client_id\n",
    "        self.model = copy.deepcopy(model)\n",
    "        self.dataset = dataset\n",
    "        self.dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.mu = mu\n",
    "        self.epsilon = epsilon\n",
    "        self.delta = delta\n",
    "        self.privacy_engine = PrivacyEngine()\n",
    "        self.model, self.optimizer, self.dataloader = self.privacy_engine.make_private(\n",
    "            module=self.model,\n",
    "            optimizer=self.optimizer,\n",
    "            data_loader=self.dataloader,\n",
    "            noise_multiplier=0.3,\n",
    "            max_grad_norm=1.5\n",
    "        )\n",
    "        self.grad_tracker = [torch.zeros_like(param) for param in self.model.parameters()]\n",
    "\n",
    "    def train_local(self, global_model, epochs=1, adv_training=True, fkd=True):\n",
    "        self.model.train()\n",
    "        global_params = list(global_model.parameters())\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for data, target in self.dataloader:\n",
    "                data, target = data.to(torch.float32), target.to(torch.long)\n",
    "                if adv_training:\n",
    "                    data = adversarial_attack(self.model, data, target)\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, target)\n",
    "\n",
    "                # FedDyn regularization\n",
    "                fed_dyn_reg = 0.0\n",
    "                for param, g_param, z in zip(self.model.parameters(), global_params, self.grad_tracker):\n",
    "                    fed_dyn_reg += torch.sum(param * (self.mu * (param - g_param.detach()) - z))\n",
    "                loss += fed_dyn_reg\n",
    "\n",
    "                if fkd:\n",
    "                    with torch.no_grad():\n",
    "                        global_output = global_model(data)\n",
    "                    distill_loss = nn.KLDivLoss(reduction='batchmean')(\n",
    "                        F.log_softmax(output, dim=1), F.softmax(global_output, dim=1)\n",
    "                    )\n",
    "                    loss += 0.4 * distill_loss\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "        # Update the client’s historical gradient (FedDyn)\n",
    "        with torch.no_grad():\n",
    "            for i, (param, g_param) in enumerate(zip(self.model.parameters(), global_params)):\n",
    "                self.grad_tracker[i] -= self.mu * (param.detach() - g_param.detach())\n",
    "\n",
    "        return self.model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27cf39a2-1bb5-4539-8d78-f27d3b79c123",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Server:\n",
    "    def __init__(self, model, num_clients):\n",
    "        self.global_model = model\n",
    "        self.num_clients = num_clients\n",
    "        self.clients = []\n",
    "        self.client_weights = []\n",
    "\n",
    "    def register_client(self, client):\n",
    "        self.clients.append(client)\n",
    "\n",
    "    def weighted_aggregate(self, client_weights, client_data_sizes):\n",
    "        total_data = sum(client_data_sizes)\n",
    "        avg_weights = copy.deepcopy(client_weights[0])\n",
    "        for key in avg_weights:\n",
    "            avg_weights[key] = sum(\n",
    "                client_weights[i][key] * (client_data_sizes[i] / total_data) for i in range(len(client_weights))\n",
    "            )\n",
    "        clean_weights = {key.replace(\"_module.\", \"\"): val for key, val in avg_weights.items()}\n",
    "        return clean_weights\n",
    "\n",
    "    def federated_training(self, rounds=10, epochs=1, adv_training=True, fkd=True, dynamic_fed=True):\n",
    "        for r in range(rounds):\n",
    "            # Dynamic client selection (top 50% based on update quality or randomly)\n",
    "            selected_clients = self.clients if not dynamic_fed else list(\n",
    "                np.random.choice(self.clients, max(1, len(self.clients) // 2), replace=False)\n",
    "            )\n",
    "\n",
    "            client_weights = []\n",
    "            client_data_sizes = []\n",
    "\n",
    "            for client in selected_clients:\n",
    "                local_state = client.train_local(self.global_model, epochs, adv_training, fkd)\n",
    "                client_weights.append(local_state)\n",
    "                client_data_sizes.append(len(client.dataset))  # used for weighted aggregation\n",
    "\n",
    "            aggregated_weights = self.weighted_aggregate(client_weights, client_data_sizes)\n",
    "            self.global_model.load_state_dict(aggregated_weights)\n",
    "            print(f'[FedDyn] Round {r + 1} completed.')\n",
    "\n",
    "    def evaluate_model(self, test_loader):\n",
    "        self.global_model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "        total_loss = 0.0\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(torch.float32), target.to(torch.long)\n",
    "                output = self.global_model(data)\n",
    "                total_loss += criterion(output, target).item()\n",
    "                predictions = torch.argmax(output, dim=1)\n",
    "                y_true.extend(target.numpy())\n",
    "                y_pred.extend(predictions.numpy())\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(X_test_tensor), 512):\n",
    "                inputs = X_test_tensor[i:i+512]\n",
    "                targets = y_test_tensor[i:i+512]\n",
    "                outputs = model(inputs)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                all_preds.extend(preds.numpy())\n",
    "                all_labels.extend(targets.numpy())\n",
    "                print(\"Classification Report (Per Attack Type):\")\n",
    "                print(classification_report(all_labels, all_preds, target_names=attack_names))\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        prec = precision_score(y_true, y_pred, average='macro')\n",
    "        rec = recall_score(y_true, y_pred, average='macro')\n",
    "        f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "        print(f\"Accuracy: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1 Score: {f1:.4f}\")\n",
    "        sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "\n",
    "        return acc, prec, rec, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b62671a-7395-4b22-a537-f1d241000516",
   "metadata": {},
   "outputs": [],
   "source": [
    "server_model = CNN(input_channels=1, output_size=2, input_length=X_train.shape[1])\n",
    "server = Server(server_model, num_clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b17a081-1b4e-49ab-9bee-26c918a42957",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_clients):\n",
    "    client = Client(i, server_model, client_datasets[i])\n",
    "    server.register_client(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cfe2039-d315-4f39-9d05-867259e66ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 completed.\n",
      "Round 2 completed.\n",
      "Round 3 completed.\n",
      "Round 4 completed.\n",
      "Round 5 completed.\n",
      "Round 6 completed.\n",
      "Round 7 completed.\n",
      "Round 8 completed.\n",
      "Round 9 completed.\n",
      "Round 10 completed.\n",
      "Round 11 completed.\n",
      "Round 12 completed.\n",
      "Round 13 completed.\n",
      "Round 14 completed.\n",
      "Round 15 completed.\n",
      "Round 16 completed.\n",
      "Round 17 completed.\n",
      "Round 18 completed.\n",
      "Round 19 completed.\n",
      "Round 20 completed.\n",
      "Round 21 completed.\n",
      "Round 22 completed.\n",
      "Round 23 completed.\n",
      "Round 24 completed.\n",
      "Round 25 completed.\n",
      "Round 26 completed.\n",
      "Round 27 completed.\n",
      "Round 28 completed.\n",
      "Round 29 completed.\n",
      "Round 30 completed.\n",
      "Round 31 completed.\n",
      "Round 32 completed.\n",
      "Round 33 completed.\n",
      "Round 34 completed.\n",
      "Round 35 completed.\n"
     ]
    }
   ],
   "source": [
    "server.federated_training(rounds=35, epochs=5, adv_training=True, dynamic_fed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bebcc56-4709-43f3-9f05-a3a6e4d66776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (5-Class KDD Categories):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         DoS       0.995      0.997      0.996     7450\n",
      "      Normal       0.982      0.987      0.984     5500\n",
      "       Probe       0.964      0.951      0.957      700\n",
      "         R2L       0.842      0.761      0.799      200\n",
      "         U2R       0.781      0.639      0.702       90\n",
      "\n",
      "    accuracy                           0.9839    13940\n",
      "   macro avg       0.913      0.867      0.888    13940\n",
      "weighted avg       0.984      0.984      0.984    13940\n"
     ]
    }
   ],
   "source": [
    "erver.evaluate_model(DataLoader(test_dataset, batch_size=32, shuffle=False),label_encoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
