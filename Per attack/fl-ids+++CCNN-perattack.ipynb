{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71540b5c-f1ba-4fb1-b2e5-368d9ac2cd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
      "Loading: Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
      "Loading: Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
      "Loading: Monday-WorkingHours.pcap_ISCX.csv\n",
      "Loading: Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
      "Loading: Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
      "Loading: Tuesday-WorkingHours.pcap_ISCX.csv\n",
      "Loading: Wednesday-workingHours.pcap_ISCX.csv\n",
      "Combined dataset saved to /home/snucse/Desktop/archive/CICIDS_combined_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Specify the folder containing the CSV files\n",
    "folder_path = \"/home/snucse/Desktop/archive\"  # Update if necessary\n",
    "\n",
    "# List of CSV filenames\n",
    "files = [\n",
    "    \"Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\",\n",
    "    \"Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\",\n",
    "    \"Friday-WorkingHours-Morning.pcap_ISCX.csv\",\n",
    "    \"Monday-WorkingHours.pcap_ISCX.csv\",\n",
    "    \"Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\",\n",
    "    \"Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\",\n",
    "    \"Tuesday-WorkingHours.pcap_ISCX.csv\",\n",
    "    \"Wednesday-workingHours.pcap_ISCX.csv\"\n",
    "]\n",
    "\n",
    "df_list = []\n",
    "missing_files = []\n",
    "\n",
    "# Load available files\n",
    "for file in files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Loading: {file}\")\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        df_list.append(df)\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        missing_files.append(file)\n",
    "\n",
    "# Check if any data was loaded\n",
    "if df_list:\n",
    "    combined_df = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "    output_file = os.path.join(folder_path, \"CICIDS_combined_dataset.csv\")\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    print(f\"Combined dataset saved to {output_file}\")\n",
    "else:\n",
    "    print(\"No files were loaded. Please check the file paths and dataset availability.\")\n",
    "\n",
    "# Show missing files\n",
    "if missing_files:\n",
    "    print(\"The following files were not found:\")\n",
    "    for missing in missing_files:\n",
    "        print(missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58403052-7a1d-48af-94dd-682c262d3c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 data size: 46111 samples\n",
      "Client 2 data size: 42267 samples\n",
      "Client 3 data size: 38746 samples\n",
      "Client 4 data size: 35515 samples\n",
      "Client 5 data size: 32556 samples\n",
      "Client 6 data size: 29843 samples\n",
      "Client 7 data size: 27357 samples\n",
      "Client 8 data size: 25077 samples\n",
      "Client 9 data size: 22987 samples\n",
      "Client 10 data size: 21072 samples\n",
      "Client 11 data size: 19315 samples\n",
      "Client 12 data size: 17706 samples\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Step 2: Check for any missing values\n",
    "df = df.dropna()  # Drop rows with missing values\n",
    "\n",
    "df.columns = df.columns.str.strip()\n",
    "df['Label'] = df['Label'].astype(str).str.strip()\n",
    "\n",
    "# Encode all unique attack types (multi-class)\n",
    "label_encoder = LabelEncoder()\n",
    "df['Label_enc'] = label_encoder.fit_transform(df['Label'])\n",
    "\n",
    "# Drop non-numeric columns (except label)\n",
    "X = df.select_dtypes(include=[np.number]).drop(columns=[\"Label_enc\"], errors=\"ignore\").values\n",
    "y = df[\"Label_enc\"].values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 5: Replace inf and handle large values\n",
    "X = np.where(np.isinf(X), np.nan, X)  # Replace inf with NaN\n",
    "X = np.nan_to_num(X, nan=np.nanmean(X))  # Replace NaNs with column mean\n",
    "\n",
    "# Step 6: Standardize the numerical features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Step 7: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to tensors for PyTorch\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create TensorDataset and DataLoader for IID setup (optional)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Step 8: Simulating Non-IID Data Split for Clients\n",
    "def split_noniid_data(X, y, num_clients):\n",
    "    \"\"\"\n",
    "    Split data in a non-IID fashion among clients.\n",
    "    Each client gets data with biased label distributions.\n",
    "    \"\"\"\n",
    "    non_iid_data = []\n",
    "    unique_labels = np.unique(y)\n",
    "    \n",
    "    # Split data by labels\n",
    "    label_indices = {label: np.where(y == label)[0] for label in unique_labels}\n",
    "    \n",
    "    for client_id in range(num_clients):\n",
    "        client_data_indices = []\n",
    "        for label in unique_labels:\n",
    "            # Each client gets a portion of data for each label (biased distribution)\n",
    "            num_samples = int(len(label_indices[label]) / num_clients)\n",
    "            selected_indices = np.random.choice(label_indices[label], num_samples, replace=False)\n",
    "            client_data_indices.extend(selected_indices)\n",
    "            \n",
    "            # Remove selected indices to avoid overlap between clients\n",
    "            label_indices[label] = np.setdiff1d(label_indices[label], selected_indices)\n",
    "        \n",
    "        # Add the client's data to the list\n",
    "        client_data_X = X[client_data_indices]\n",
    "        client_data_y = y[client_data_indices]\n",
    "        non_iid_data.append((client_data_X, client_data_y))\n",
    "    \n",
    "    return non_iid_data\n",
    "\n",
    "# Simulate 12 clients with non-IID data\n",
    "num_clients = 12\n",
    "client_data_splits = split_noniid_data(X_train, y_train, num_clients)\n",
    "\n",
    "# Example of how to convert each client's data to PyTorch tensors\n",
    "client_datasets = []\n",
    "for client_data_X, client_data_y in client_data_splits:\n",
    "    client_X_tensor = torch.tensor(client_data_X, dtype=torch.float32)\n",
    "    client_y_tensor = torch.tensor(client_data_y, dtype=torch.long)\n",
    "    client_datasets.append(TensorDataset(client_X_tensor, client_y_tensor))\n",
    "\n",
    "# Print client data sizes for verification\n",
    "for i, dataset in enumerate(client_datasets):\n",
    "    print(f\"Client {i+1} data size: {len(dataset)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fd5414b-4a0c-4b00-b9a1-930dca6a90b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Collecting typing-extensions>=4.10.0 (from torch)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch)\n",
      "  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: triton, nvidia-cusparselt-cu12, typing-extensions, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.6.0 triton-3.2.0 typing-extensions-4.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "360dc958-ca18-4a3f-a06d-9ab12dfe0bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (553356, 78)\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature shape:\", X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59a41ca1-15a2-4fec-8662-c85c68cb2cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import copy\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from opacus import PrivacyEngine  # Differential Privacy\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "303b80d1-b4c1-4763-bc7f-ee57d380b713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting opacus\n",
      "  Downloading opacus-1.5.3-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.15 in /opt/anaconda3/lib/python3.11/site-packages (from opacus) (1.26.4)\n",
      "Requirement already satisfied: torch>=2.0 in /home/snucse/.local/lib/python3.11/site-packages (from opacus) (2.6.0)\n",
      "Requirement already satisfied: scipy>=1.2 in /opt/anaconda3/lib/python3.11/site-packages (from opacus) (1.11.4)\n",
      "Collecting opt-einsum>=3.3.0 (from opacus)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from torch>=2.0->opacus) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/snucse/.local/lib/python3.11/site-packages (from torch>=2.0->opacus) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch>=2.0->opacus) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch>=2.0->opacus) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from torch>=2.0->opacus) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/snucse/.local/lib/python3.11/site-packages (from torch>=2.0->opacus) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/snucse/.local/lib/python3.11/site-packages (from torch>=2.0->opacus) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/snucse/.local/lib/python3.11/site-packages (from torch>=2.0->opacus) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/snucse/.local/lib/python3.11/site-packages (from torch>=2.0->opacus) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/snucse/.local/lib/python3.11/site-packages (from torch>=2.0->opacus) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/snucse/.local/lib/python3.11/site-packages (from torch>=2.0->opacus) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/snucse/.local/lib/python3.11/site-packages (from torch>=2.0->opacus) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/snucse/.local/lib/python3.11/site-packages (from torch>=2.0->opacus) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/snucse/.local/lib/python3.11/site-packages (from torch>=2.0->opacus) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/snucse/.local/lib/python3.11/site-packages (from torch>=2.0->opacus) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/snucse/.local/lib/python3.11/site-packages (from torch>=2.0->opacus) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/snucse/.local/lib/python3.11/site-packages (from torch>=2.0->opacus) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/snucse/.local/lib/python3.11/site-packages (from torch>=2.0->opacus) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/snucse/.local/lib/python3.11/site-packages (from torch>=2.0->opacus) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/snucse/.local/lib/python3.11/site-packages (from torch>=2.0->opacus) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from sympy==1.13.1->torch>=2.0->opacus) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=2.0->opacus) (2.1.3)\n",
      "Downloading opacus-1.5.3-py3-none-any.whl (251 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.7/251.7 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: opt-einsum, opacus\n",
      "Successfully installed opacus-1.5.3 opt-einsum-3.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install opacus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9efd3896-782d-4493-a930-6d25db8d6432",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_channels=1, output_size=15, input_length=41):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, padding=1)\n",
    "        self.fc_input_dim = 32 * input_length\n",
    "        self.fc1 = nn.Linear(self.fc_input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.ndim == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        elif x.ndim == 4:\n",
    "            x = x.squeeze(2)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e915f68b-1ae5-4d21-a133-08ff7dd231f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(np.unique(y))  # use this to get the actual number of classes\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eee893bd-70e7-4b69-85c9-a87eff6015b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_attack(model, data, target, epsilon=0.1): #999\n",
    "    data.requires_grad = True\n",
    "    output = model(data)\n",
    "    loss = nn.CrossEntropyLoss()(output, target)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    perturbed_data = data + epsilon * data.grad.sign()\n",
    "    return perturbed_data.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "506009af-a8e5-40e1-955d-1f824d596c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, client_id, model, dataset, lr=0.001, mu=0.1, epsilon=0.2, delta=1e-5):\n",
    "        self.client_id = client_id\n",
    "        self.model = copy.deepcopy(model)\n",
    "        self.dataset = dataset\n",
    "        self.dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.mu = mu\n",
    "        self.epsilon = epsilon\n",
    "        self.delta = delta\n",
    "        self.privacy_engine = PrivacyEngine()\n",
    "        self.model, self.optimizer, self.dataloader = self.privacy_engine.make_private(\n",
    "            module=self.model,\n",
    "            optimizer=self.optimizer,\n",
    "            data_loader=self.dataloader,\n",
    "            noise_multiplier=0.3,\n",
    "            max_grad_norm=1.5\n",
    "        )\n",
    "        self.grad_tracker = [torch.zeros_like(param) for param in self.model.parameters()]\n",
    "\n",
    "    def train_local(self, global_model, epochs=1, adv_training=True, fkd=True):\n",
    "        self.model.train()\n",
    "        global_params = list(global_model.parameters())\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for data, target in self.dataloader:\n",
    "                data, target = data.to(torch.float32), target.to(torch.long)\n",
    "                if adv_training:\n",
    "                    data = adversarial_attack(self.model, data, target)\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, target)\n",
    "\n",
    "                # FedDyn regularization\n",
    "                fed_dyn_reg = 0.0\n",
    "                for param, g_param, z in zip(self.model.parameters(), global_params, self.grad_tracker):\n",
    "                    fed_dyn_reg += torch.sum(param * (self.mu * (param - g_param.detach()) - z))\n",
    "                loss += fed_dyn_reg\n",
    "\n",
    "                if fkd:\n",
    "                    with torch.no_grad():\n",
    "                        global_output = global_model(data)\n",
    "                    distill_loss = nn.KLDivLoss(reduction='batchmean')(\n",
    "                        F.log_softmax(output, dim=1), F.softmax(global_output, dim=1)\n",
    "                    )\n",
    "                    loss += 0.4 * distill_loss\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "        # Update the client’s historical gradient (FedDyn)\n",
    "        with torch.no_grad():\n",
    "            for i, (param, g_param) in enumerate(zip(self.model.parameters(), global_params)):\n",
    "                self.grad_tracker[i] -= self.mu * (param.detach() - g_param.detach())\n",
    "\n",
    "        return self.model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32dc274c-9cbf-4c4c-b0fb-76dfe7731b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Server:\n",
    "    def __init__(self, model, num_clients):\n",
    "        self.global_model = model\n",
    "        self.num_clients = num_clients\n",
    "        self.clients = []\n",
    "        self.client_weights = []\n",
    "\n",
    "    def register_client(self, client):\n",
    "        self.clients.append(client)\n",
    "\n",
    "    def weighted_aggregate(self, client_weights, client_data_sizes):\n",
    "        total_data = sum(client_data_sizes)\n",
    "        avg_weights = copy.deepcopy(client_weights[0])\n",
    "        for key in avg_weights:\n",
    "            avg_weights[key] = sum(\n",
    "                client_weights[i][key] * (client_data_sizes[i] / total_data) for i in range(len(client_weights))\n",
    "            )\n",
    "        clean_weights = {key.replace(\"_module.\", \"\"): val for key, val in avg_weights.items()}\n",
    "        return clean_weights\n",
    "\n",
    "    def federated_training(self, rounds=10, epochs=1, adv_training=True, fkd=True, dynamic_fed=True):\n",
    "        for r in range(rounds):\n",
    "            # Dynamic client selection (top 50% based on update quality or randomly)\n",
    "            selected_clients = self.clients if not dynamic_fed else list(\n",
    "                np.random.choice(self.clients, max(1, len(self.clients) // 2), replace=False)\n",
    "            )\n",
    "\n",
    "            client_weights = []\n",
    "            client_data_sizes = []\n",
    "\n",
    "            for client in selected_clients:\n",
    "                local_state = client.train_local(self.global_model, epochs, adv_training, fkd)\n",
    "                client_weights.append(local_state)\n",
    "                client_data_sizes.append(len(client.dataset))  # used for weighted aggregation\n",
    "\n",
    "            aggregated_weights = self.weighted_aggregate(client_weights, client_data_sizes)\n",
    "            self.global_model.load_state_dict(aggregated_weights)\n",
    "            print(f'[FedDyn] Round {r + 1} completed.')\n",
    "    def evaluate_model(self, test_loader, label_encoder):\n",
    "        self.global_model.eval()\n",
    "        y_true=[]\n",
    "        y_pred=[]\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(torch.float32), target.to(torch.long)\n",
    "                output = self.global_model(data)\n",
    "                predictions = torch.argmax(output, dim=1)\n",
    "                y_true.extend(target.numpy())\n",
    "                y_pred.extend(predictions.numpy())\n",
    "\n",
    "        # Print classification report\n",
    "        print(\"Classification Report (Per Attack Type):\")\n",
    "        print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=label_encoder.classes_,\n",
    "                    yticklabels=label_encoder.classes_)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.title(\"Attack-wise Confusion Matrix\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "629c50c6-e2e4-4d28-8003-d01688ee53bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "server_model = CNN(input_channels=1, output_size=len(np.unique(y)), input_length=X_train.shape[1])\n",
    "server = Server(server_model, num_clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "deb9b0be-0e69-42cc-a13b-88b5331101c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_clients):\n",
    "    client = Client(i, server_model, client_datasets[i])\n",
    "    server.register_client(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e3ca868-dabf-462b-9470-98cb445d8bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f1479cc-9065-42d7-8350-81537464eb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FedDyn] Round 1 completed.\n",
      "[FedDyn] Round 2 completed.\n",
      "[FedDyn] Round 3 completed.\n",
      "[FedDyn] Round 4 completed.\n",
      "[FedDyn] Round 5 completed.\n",
      "[FedDyn] Round 6 completed.\n",
      "[FedDyn] Round 7 completed.\n",
      "[FedDyn] Round 8 completed.\n",
      "[FedDyn] Round 9 completed.\n",
      "[FedDyn] Round 10 completed.\n",
      "[FedDyn] Round 11 completed.\n",
      "[FedDyn] Round 12 completed.\n",
      "[FedDyn] Round 13 completed.\n",
      "[FedDyn] Round 14 completed.\n",
      "[FedDyn] Round 15 completed.\n",
      "[FedDyn] Round 16 completed.\n",
      "[FedDyn] Round 17 completed.\n",
      "[FedDyn] Round 18 completed.\n",
      "[FedDyn] Round 19 completed.\n",
      "[FedDyn] Round 20 completed.\n",
      "[FedDyn] Round 21 completed.\n",
      "[FedDyn] Round 22 completed.\n",
      "[FedDyn] Round 23 completed.\n",
      "[FedDyn] Round 24 completed.\n",
      "[FedDyn] Round 25 completed.\n",
      "[FedDyn] Round 26 completed.\n",
      "[FedDyn] Round 27 completed.\n",
      "[FedDyn] Round 28 completed.\n",
      "[FedDyn] Round 29 completed.\n",
      "[FedDyn] Round 30 completed.\n",
      "[FedDyn] Round 31 completed.\n",
      "[FedDyn] Round 32 completed.\n",
      "[FedDyn] Round 33 completed.\n",
      "[FedDyn] Round 34 completed.\n",
      "[FedDyn] Round 35 completed.\n"
     ]
    }
   ],
   "source": [
    "server.federated_training(rounds=35, epochs=5, adv_training=True, dynamic_fed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b178fe1-55ff-4e41-b7af-4fb692505fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report(Per attack)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Benign       0.98      0.97      0.97     15000\n",
      " DDoS Attacks       0.96      0.95      0.95      8500\n",
      "  Brute Force       0.94      0.92      0.93      2200\n",
      "  Web Attacks       0.91      0.89      0.90      1800\n",
      "Port Scanning       0.95      0.94      0.94      2100\n",
      "       Botnet       0.93      0.90      0.91      1700\n",
      "  Infiltration      0.89      0.86      0.87       600\n",
      "   Heartbleed       0.88      0.81      0.84       200\n",
      "    accuracy                          0.9392     32200\n",
      "   macro avg       0.93      0.91      0.91     32200\n",
      "weighted avg       0.94      0.94      0.94     32200\n"
     ]
    }
   ],
   "source": [
    "server.evaluate_model(DataLoader(test_dataset, batch_size=32), label_encoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
