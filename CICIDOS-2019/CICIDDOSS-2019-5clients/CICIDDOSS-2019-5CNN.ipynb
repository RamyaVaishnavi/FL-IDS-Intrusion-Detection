{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8c5506e-dcc4-4b57-8d90-a51ac9006641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training files: ['MSSQL-training.parquet', 'NetBIOS-training.parquet', 'UDP-training.parquet', 'LDAP-training.parquet', 'Syn-training.parquet', 'Portmap-training.parquet', 'UDPLag-training.parquet']\n",
      "Testing files: ['DNS-testing.parquet', 'LDAP-testing.parquet', 'UDPLag-testing.parquet', 'MSSQL-testing.parquet', 'NetBIOS-testing.parquet', 'NTP-testing.parquet', 'UDP-testing.parquet', 'TFTP-testing.parquet', 'Syn-testing.parquet', 'SNMP-testing.parquet']\n",
      "Loading training file: MSSQL-training.parquet\n",
      "Loading training file: NetBIOS-training.parquet\n",
      "Loading training file: UDP-training.parquet\n",
      "Loading training file: LDAP-training.parquet\n",
      "Loading training file: Syn-training.parquet\n",
      "Loading training file: Portmap-training.parquet\n",
      "Loading training file: UDPLag-training.parquet\n",
      "Loading testing file: DNS-testing.parquet\n",
      "Loading testing file: LDAP-testing.parquet\n",
      "Loading testing file: UDPLag-testing.parquet\n",
      "Loading testing file: MSSQL-testing.parquet\n",
      "Loading testing file: NetBIOS-testing.parquet\n",
      "Loading testing file: NTP-testing.parquet\n",
      "Loading testing file: UDP-testing.parquet\n",
      "Loading testing file: TFTP-testing.parquet\n",
      "Loading testing file: Syn-testing.parquet\n",
      "Loading testing file: SNMP-testing.parquet\n",
      "Training shape: (125170, 78)\n",
      "Testing shape: (306201, 78)\n",
      "Client 1 data size: 25033 samples\n",
      "Client 2 data size: 20027 samples\n",
      "Client 3 data size: 16021 samples\n",
      "Client 4 data size: 12817 samples\n",
      "Client 5 data size: 10253 samples\n",
      "Test dataset size: 306201 samples\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# ============================\n",
    "# Step 1: Load CICDDoS-2019 Parquet Files\n",
    "# ============================\n",
    "\n",
    "folder_path = \"/home/snucse/Documents/CICIDDOSS-2019/\"  # <-- update path\n",
    "\n",
    "# Collect all parquet files\n",
    "all_files = [f for f in os.listdir(folder_path) if f.endswith(\".parquet\")]\n",
    "\n",
    "train_files = [f for f in all_files if \"training\" in f.lower()]\n",
    "test_files = [f for f in all_files if \"testing\" in f.lower()]\n",
    "\n",
    "print(\"Training files:\", train_files)\n",
    "print(\"Testing files:\", test_files)\n",
    "\n",
    "# Load training files\n",
    "df_train_list = []\n",
    "for file in train_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    print(f\"Loading training file: {file}\")\n",
    "    df_train_list.append(pd.read_parquet(file_path))\n",
    "\n",
    "df_train = pd.concat(df_train_list, axis=0, ignore_index=True)\n",
    "\n",
    "# Load testing files\n",
    "df_test_list = []\n",
    "for file in test_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    print(f\"Loading testing file: {file}\")\n",
    "    df_test_list.append(pd.read_parquet(file_path))\n",
    "\n",
    "df_test = pd.concat(df_test_list, axis=0, ignore_index=True)\n",
    "\n",
    "print(f\"Training shape: {df_train.shape}\")\n",
    "print(f\"Testing shape: {df_test.shape}\")\n",
    "\n",
    "# ============================\n",
    "# Step 2: Preprocessing\n",
    "# ============================\n",
    "\n",
    "# Drop NA values\n",
    "df_train = df_train.dropna()\n",
    "df_test = df_test.dropna()\n",
    "\n",
    "# Map labels: BENIGN = 0, others = 1 (binary classification)\n",
    "df_train['Label'] = df_train['Label'].apply(lambda x: 0 if x.upper() == 'BENIGN' else 1)\n",
    "df_test['Label'] = df_test['Label'].apply(lambda x: 0 if x.upper() == 'BENIGN' else 1)\n",
    "\n",
    "# Split features & labels\n",
    "X_train = df_train.drop(columns=['Label']).values\n",
    "y_train = df_train['Label'].values\n",
    "\n",
    "X_test = df_test.drop(columns=['Label']).values\n",
    "y_test = df_test['Label'].values\n",
    "\n",
    "# Replace inf/nan\n",
    "X_train = np.where(np.isinf(X_train), np.nan, X_train)\n",
    "X_test = np.where(np.isinf(X_test), np.nan, X_test)\n",
    "\n",
    "train_means = np.nanmean(X_train, axis=0)\n",
    "test_means = np.nanmean(X_test, axis=0)\n",
    "\n",
    "inds_train = np.where(np.isnan(X_train))\n",
    "inds_test = np.where(np.isnan(X_test))\n",
    "\n",
    "X_train[inds_train] = np.take(train_means, inds_train[1])\n",
    "X_test[inds_test] = np.take(test_means, inds_test[1])\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# ============================\n",
    "# Step 3: Non-IID Split (5 clients)\n",
    "# ============================\n",
    "\n",
    "def split_noniid_data(X, y, num_clients):\n",
    "    non_iid_data = []\n",
    "    unique_labels = np.unique(y)\n",
    "    label_indices = {label: np.where(y == label)[0] for label in unique_labels}\n",
    "\n",
    "    for client_id in range(num_clients):\n",
    "        client_data_indices = []\n",
    "        for label in unique_labels:\n",
    "            num_samples = int(len(label_indices[label]) / num_clients)\n",
    "            if num_samples > 0:\n",
    "                selected_indices = np.random.choice(label_indices[label], num_samples, replace=False)\n",
    "                client_data_indices.extend(selected_indices)\n",
    "                label_indices[label] = np.setdiff1d(label_indices[label], selected_indices)\n",
    "\n",
    "        client_data_X = X[client_data_indices]\n",
    "        client_data_y = y[client_data_indices]\n",
    "        non_iid_data.append((client_data_X, client_data_y))\n",
    "\n",
    "    return non_iid_data\n",
    "\n",
    "num_clients = 5\n",
    "client_data_splits = split_noniid_data(X_train, y_train, num_clients)\n",
    "\n",
    "# Convert each client's data to TensorDataset\n",
    "client_datasets = []\n",
    "for client_data_X, client_data_y in client_data_splits:\n",
    "    client_X_tensor = torch.tensor(client_data_X, dtype=torch.float32)\n",
    "    client_y_tensor = torch.tensor(client_data_y, dtype=torch.long)\n",
    "    client_datasets.append(TensorDataset(client_X_tensor, client_y_tensor))\n",
    "\n",
    "# ============================\n",
    "# Step 4: Verification\n",
    "# ============================\n",
    "\n",
    "for i, dataset in enumerate(client_datasets):\n",
    "    print(f\"Client {i+1} data size: {len(dataset)} samples\")\n",
    "\n",
    "print(f\"Test dataset size: {len(test_dataset)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb4ac192-e35e-4bb4-8197-68591824eb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import copy\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from opacus import PrivacyEngine\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b03fa053-a2c0-4fbb-b70c-a6b0247e51cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_channels=1, output_size=2, input_length=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Compute the correct flattened size\n",
    "        self.input_length = input_length\n",
    "        dummy_input = torch.zeros(1, input_channels, input_length)\n",
    "        dummy_output = self.conv2(self.conv1(dummy_input))\n",
    "        self.flattened_size = dummy_output.numel() // dummy_output.shape[0]\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.flattened_size, 128)\n",
    "        self.fc2 = nn.Linear(128, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten dynamically\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78cc1c3f-6ed2-49fc-9d47-f1a8cec9b10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_attack(model, data, target, epsilon=0.1):\n",
    "    data.requires_grad = True\n",
    "    output = model(data)\n",
    "    loss = nn.CrossEntropyLoss()(output, target)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    perturbed_data = data + epsilon * data.grad.sign()\n",
    "    return perturbed_data.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc4ce6a8-b8a3-4f20-8096-4f8a354c9b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, client_id, model, dataset, lr=0.001, mu=0.1, epsilon=0.2, delta=1e-5):\n",
    "        self.client_id = client_id\n",
    "        self.model = copy.deepcopy(model)\n",
    "        self.dataset = dataset\n",
    "        self.dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.mu = mu\n",
    "        self.epsilon = epsilon\n",
    "        self.delta = delta\n",
    "        self.privacy_engine = PrivacyEngine()\n",
    "        self.model, self.optimizer, self.dataloader = self.privacy_engine.make_private(\n",
    "            module=self.model,\n",
    "            optimizer=self.optimizer,\n",
    "            data_loader=self.dataloader,\n",
    "            noise_multiplier=0.3,\n",
    "            max_grad_norm=1.5\n",
    "        )\n",
    "        self.grad_tracker = [torch.zeros_like(param) for param in self.model.parameters()]\n",
    "\n",
    "    def train_local(self, global_model, epochs=1, adv_training=True, fkd=True):\n",
    "        self.model.train()\n",
    "        global_params = list(global_model.parameters())\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for data, target in self.dataloader:\n",
    "                data, target = data.to(torch.float32), target.to(torch.long)\n",
    "                if adv_training:\n",
    "                    data = adversarial_attack(self.model, data, target)\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, target)\n",
    "\n",
    "                # FedDyn regularization\n",
    "                fed_dyn_reg = 0.0\n",
    "                for param, g_param, z in zip(self.model.parameters(), global_params, self.grad_tracker):\n",
    "                    fed_dyn_reg += torch.sum(param * (self.mu * (param - g_param.detach()) - z))\n",
    "                loss += fed_dyn_reg\n",
    "\n",
    "                if fkd:\n",
    "                    with torch.no_grad():\n",
    "                        global_output = global_model(data)\n",
    "                    distill_loss = nn.KLDivLoss(reduction='batchmean')(\n",
    "                        F.log_softmax(output, dim=1), F.softmax(global_output, dim=1)\n",
    "                    )\n",
    "                    loss += 0.4 * distill_loss\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "        # Update the client’s historical gradient (FedDyn)\n",
    "        with torch.no_grad():\n",
    "            for i, (param, g_param) in enumerate(zip(self.model.parameters(), global_params)):\n",
    "                self.grad_tracker[i] -= self.mu * (param.detach() - g_param.detach())\n",
    "\n",
    "        return self.model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a685fbf0-1433-4499-a0ec-b54256546b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Server:\n",
    "    def __init__(self, model, num_clients, mu=0.1):\n",
    "        self.global_model = model\n",
    "        self.num_clients = num_clients\n",
    "        self.clients = []\n",
    "        self.mu = mu\n",
    "        self.h_dict = {}  # Drift term for FedDyn\n",
    "\n",
    "    def register_client(self, client):\n",
    "        self.clients.append(client)\n",
    "        self.h_dict[client.client_id] = {k: torch.zeros_like(v) for k, v in self.global_model.state_dict().items()}\n",
    "\n",
    "    def aggregate_weights_feddyn(self, client_updates):\n",
    "        new_global_weights = copy.deepcopy(self.global_model.state_dict())\n",
    "\n",
    "        for key in new_global_weights.keys():\n",
    "            avg_update = torch.stack([update[0][key] - (1 / self.mu) * update[1][key] for update in client_updates])\n",
    "            new_global_weights[key] = avg_update.mean(dim=0)\n",
    "\n",
    "        return new_global_weights\n",
    "\n",
    "    def federated_training(self, rounds=10, epochs=1, adv_training=True, fkd=True):\n",
    "        for r in range(rounds):\n",
    "            # Adaptive client selection based on drift norm (FedDyn)\n",
    "            drift_norms = {client.client_id: torch.norm(torch.cat([v.view(-1) for v in self.h_dict[client.client_id].values()]))\n",
    "                           for client in self.clients}\n",
    "            sorted_clients = sorted(self.clients, key=lambda x: drift_norms[x.client_id], reverse=True)\n",
    "            selected_clients = sorted_clients[:max(1, len(self.clients) // 2)]\n",
    "\n",
    "            print(f\"Round {r+1} | Selected Clients: {[c.client_id for c in selected_clients]}\")\n",
    "\n",
    "            client_updates = []\n",
    "            for client in selected_clients:\n",
    "                state_dict, h_new = client.train_local(self.global_model, self.h_dict[client.client_id], epochs, adv_training, fkd)\n",
    "                client_updates.append((state_dict, self.h_dict[client.client_id]))\n",
    "                self.h_dict[client.client_id] = h_new\n",
    "\n",
    "            # FedDyn Aggregation\n",
    "            new_weights = self.aggregate_weights_feddyn(client_updates)\n",
    "            self.global_model.load_state_dict(new_weights)\n",
    "\n",
    "    def evaluate_model(self, test_loader):\n",
    "        self.global_model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "        total_loss = 0.0\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(torch.float32), target.to(torch.long)\n",
    "                output = self.global_model(data)\n",
    "                loss = criterion(output, target)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                predictions = torch.argmax(output, dim=1)\n",
    "                y_true.extend(target.numpy())\n",
    "                y_pred.extend(predictions.numpy())\n",
    "\n",
    "        # Evaluation Metrics\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred, average='macro')\n",
    "        recall = recall_score(y_true, y_pred, average='macro')\n",
    "        f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "        print(\"Evaluation Metrics:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "\n",
    "        return accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757ea396-1c5a-46d9-8e6e-5d3c5a11f4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[1]\n",
    "server_model = CNN(input_size)\n",
    "server = Server(server_model, num_clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecdb35a-ec77-4bd5-a41f-883b62369a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_clients):\n",
    "    client = Client(i, server_model, client_datasets[i])\n",
    "    server.register_client(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aec8049b-b459-4d0a-99e2-247162f3a4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 completed.\n",
      "Round 2 completed.\n",
      "Round 3 completed.\n",
      "Round 4 completed.\n",
      "Round 5 completed.\n",
      "Round 6 completed.\n",
      "Round 7 completed.\n",
      "Round 8 completed.\n",
      "Round 9 completed.\n",
      "Round 10 completed.\n",
      "Round 11 completed.\n",
      "Round 12 completed.\n",
      "Round 13 completed.\n",
      "Round 14 completed.\n",
      "Round 15 completed.\n",
      "Round 16 completed.\n",
      "Round 17 completed.\n",
      "Round 18 completed.\n",
      "Round 19 completed.\n",
      "Round 20 completed.\n",
      "Round 21 completed.\n",
      "Round 22 completed.\n",
      "Round 23 completed.\n",
      "Round 24 completed.\n",
      "Round 25 completed.\n",
      "Round 26 completed.\n",
      "Round 27 completed.\n",
      "Round 28 completed.\n",
      "Round 29 completed.\n",
      "Round 30 completed.\n",
      "Round 31 completed.\n",
      "Round 32 completed.\n",
      "Round 33 completed.\n",
      "Round 34 completed.\n",
      "Round 35 completed.\n"
     ]
    }
   ],
   "source": [
    "server.federated_training(rounds=35, epochs=5, adv_training=True, dynamic_fed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dac77f1-d949-402f-8b5f-0c226b3aaf78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy:0.9365\n",
      "Recall:0.9158\n",
      "Precision:0.9221\n"
     ]
    }
   ],
   "source": [
    "server.evaluate_model(DataLoader(test_dataset, batch_size=32, shuffle=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
