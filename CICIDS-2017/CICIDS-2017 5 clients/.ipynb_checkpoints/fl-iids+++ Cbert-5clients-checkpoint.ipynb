{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05107594-b506-4a68-ba63-aa0fa692f0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Destination Port   Flow Duration   Total Fwd Packets  \\\n",
      "0              54865               3                   2   \n",
      "1              55054             109                   1   \n",
      "2              55055              52                   1   \n",
      "3              46236              34                   1   \n",
      "4              54863               3                   2   \n",
      "\n",
      "    Total Backward Packets  Total Length of Fwd Packets  \\\n",
      "0                        0                           12   \n",
      "1                        1                            6   \n",
      "2                        1                            6   \n",
      "3                        1                            6   \n",
      "4                        0                           12   \n",
      "\n",
      "    Total Length of Bwd Packets   Fwd Packet Length Max  \\\n",
      "0                             0                       6   \n",
      "1                             6                       6   \n",
      "2                             6                       6   \n",
      "3                             6                       6   \n",
      "4                             0                       6   \n",
      "\n",
      "    Fwd Packet Length Min   Fwd Packet Length Mean   Fwd Packet Length Std  \\\n",
      "0                       6                      6.0                     0.0   \n",
      "1                       6                      6.0                     0.0   \n",
      "2                       6                      6.0                     0.0   \n",
      "3                       6                      6.0                     0.0   \n",
      "4                       6                      6.0                     0.0   \n",
      "\n",
      "   ...   min_seg_size_forward  Active Mean   Active Std   Active Max  \\\n",
      "0  ...                     20          0.0          0.0            0   \n",
      "1  ...                     20          0.0          0.0            0   \n",
      "2  ...                     20          0.0          0.0            0   \n",
      "3  ...                     20          0.0          0.0            0   \n",
      "4  ...                     20          0.0          0.0            0   \n",
      "\n",
      "    Active Min  Idle Mean   Idle Std   Idle Max   Idle Min   Label  \n",
      "0            0        0.0        0.0          0          0  BENIGN  \n",
      "1            0        0.0        0.0          0          0  BENIGN  \n",
      "2            0        0.0        0.0          0          0  BENIGN  \n",
      "3            0        0.0        0.0          0          0  BENIGN  \n",
      "4            0        0.0        0.0          0          0  BENIGN  \n",
      "\n",
      "[5 rows x 79 columns]\n",
      "Combined dataset saved to D:\\federated learning\\data\\cicid\\CICIDS_combined_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Specify the folder containing the CSV files\n",
    "folder_path = r'D:\\federated learning\\data\\cicid'  # Replace with the path to your dataset\n",
    "\n",
    "# List of files to load (update these based on your files)\n",
    "files = [\n",
    "    r'D:\\federated learning\\data\\cicid\\Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv',\n",
    "    r'D:\\federated learning\\data\\cicid\\Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv',\n",
    "    r'D:\\federated learning\\data\\cicid\\Friday-WorkingHours-Morning.pcap_ISCX.csv',\n",
    "    r'D:\\federated learning\\data\\cicid\\Monday-WorkingHours.pcap_ISCX.csv',\n",
    "    r'D:\\federated learning\\data\\cicid\\Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv',\n",
    "    r'D:\\federated learning\\data\\cicid\\Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv',\n",
    "    r'D:\\federated learning\\data\\cicid\\Tuesday-WorkingHours.pcap_ISCX.csv',\n",
    "    r'D:\\federated learning\\data\\cicid\\Wednesday-WorkingHours.pcap_ISCX.csv'\n",
    "]\n",
    "\n",
    "df_list = []\n",
    "for file in files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_csv(file_path, low_memory=False)  # Read each file\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "combined_df = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "\n",
    "# Display the first few rows\n",
    "print(combined_df.head())\n",
    "output_file = r\"D:\\federated learning\\data\\cicid\\CICIDS_combined_dataset.csv\"\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Combined dataset saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "977e42c9-f2f7-4916-a248-64510fa12343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_13848\\1255895172.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[' Label'] = df[' Label'].apply(lambda x: 1 if x != 'BENIGN' else 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 data size: 110671 samples\n",
      "Client 2 data size: 88536 samples\n",
      "Client 3 data size: 70829 samples\n",
      "Client 4 data size: 56664 samples\n",
      "Client 5 data size: 45330 samples\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Step 2: Check for any missing values\n",
    "df = df.dropna()  # Drop rows with missing values\n",
    "\n",
    "# Step 3: Encode the 'Label' column (normal vs attack)\n",
    "df[' Label'] = df[' Label'].apply(lambda x: 1 if x != 'BENIGN' else 0)\n",
    "\n",
    "# Step 4: Select relevant features (drop 'Label' as it's the target)\n",
    "X = df.drop(' Label', axis=1).values\n",
    "y = df[' Label'].values\n",
    "\n",
    "# Step 5: Replace inf and handle large values\n",
    "X = np.where(np.isinf(X), np.nan, X)  # Replace inf with NaN\n",
    "X = np.nan_to_num(X, nan=np.nanmean(X))  # Replace NaNs with column mean\n",
    "\n",
    "# Step 6: Standardize the numerical features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Step 7: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to tensors for PyTorch\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create TensorDataset and DataLoader for IID setup (optional)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Step 8: Simulating Non-IID Data Split for Clients\n",
    "def split_noniid_data(X, y, num_clients):\n",
    "    \"\"\"\n",
    "    Split data in a non-IID fashion among clients.\n",
    "    Each client gets data with biased label distributions.\n",
    "    \"\"\"\n",
    "    non_iid_data = []\n",
    "    unique_labels = np.unique(y)\n",
    "    \n",
    "    # Split data by labels\n",
    "    label_indices = {label: np.where(y == label)[0] for label in unique_labels}\n",
    "    \n",
    "    for client_id in range(num_clients):\n",
    "        client_data_indices = []\n",
    "        for label in unique_labels:\n",
    "            # Each client gets a portion of data for each label (biased distribution)\n",
    "            num_samples = int(len(label_indices[label]) / num_clients)\n",
    "            selected_indices = np.random.choice(label_indices[label], num_samples, replace=False)\n",
    "            client_data_indices.extend(selected_indices)\n",
    "            \n",
    "            # Remove selected indices to avoid overlap between clients\n",
    "            label_indices[label] = np.setdiff1d(label_indices[label], selected_indices)\n",
    "        \n",
    "        # Add the client's data to the list\n",
    "        client_data_X = X[client_data_indices]\n",
    "        client_data_y = y[client_data_indices]\n",
    "        non_iid_data.append((client_data_X, client_data_y))\n",
    "    \n",
    "    return non_iid_data\n",
    "\n",
    "num_clients = 5\n",
    "client_data_splits = split_noniid_data(X_train, y_train, num_clients)\n",
    "\n",
    "# Example of how to convert each client's data to PyTorch tensors\n",
    "client_datasets = []\n",
    "for client_data_X, client_data_y in client_data_splits:\n",
    "    client_X_tensor = torch.tensor(client_data_X, dtype=torch.float32)\n",
    "    client_y_tensor = torch.tensor(client_data_y, dtype=torch.long)\n",
    "    client_datasets.append(TensorDataset(client_X_tensor, client_y_tensor))\n",
    "\n",
    "# Print client data sizes for verification\n",
    "for i, dataset in enumerate(client_datasets):\n",
    "    print(f\"Client {i+1} data size: {len(dataset)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bea3d903-6f62-4e34-ad56-2886569c7938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import copy\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from opacus import PrivacyEngine\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b6e5b8a-138f-435d-8fbf-912853141955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel, DistilBertConfig\n",
    "\n",
    "class TabularFeatureEmbedder(nn.Module):\n",
    "    def __init__(self, input_dim=41, seq_len=41, embed_dim=768):\n",
    "        super(TabularFeatureEmbedder, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, seq_len * embed_dim)\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        return x.view(-1, self.seq_len, self.embed_dim)\n",
    "\n",
    "class DistilBERTIntrusionClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size=768, output_size=2):\n",
    "        super(DistilBERTIntrusionClassifier, self).__init__()\n",
    "        config = DistilBertConfig(\n",
    "            dim=hidden_size,\n",
    "            hidden_dim=1024,\n",
    "            n_layers=4,\n",
    "            n_heads=4,\n",
    "            dropout=0.1,\n",
    "            attention_dropout=0.1\n",
    "        )\n",
    "        self.bert = DistilBertModel(config)\n",
    "        self.classifier = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_mask = torch.ones(x.size(0), x.size(1)).to(x.device)\n",
    "        x = self.bert(inputs_embeds=x, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        return self.classifier(x)\n",
    "\n",
    "class CombinedDistilBERTModel(nn.Module):\n",
    "    def __init__(self, embedder, classifier):\n",
    "        super(CombinedDistilBERTModel, self).__init__()\n",
    "        self.embedder = embedder\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedder(x)\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7cd2c65-3f7e-4a89-91bf-67ec409b45b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_attack(model, data, target, epsilon=0.1):\n",
    "    data.requires_grad = True\n",
    "    output = model(data)\n",
    "    loss = nn.CrossEntropyLoss()(output, target)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    perturbed_data = data + epsilon * data.grad.sign()\n",
    "    return perturbed_data.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1517f6cb-a3f9-46d6-b522-7255a8232df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, client_id, model, dataset, lr=0.001, mu=0.1, epsilon=0.2, delta=1e-5):\n",
    "        self.client_id = client_id\n",
    "        self.model = copy.deepcopy(model)\n",
    "        self.dataset = dataset\n",
    "        self.dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.mu = mu\n",
    "        self.epsilon = epsilon\n",
    "        self.delta = delta\n",
    "        self.privacy_engine = PrivacyEngine()\n",
    "        self.model, self.optimizer, self.dataloader = self.privacy_engine.make_private(\n",
    "            module=self.model,\n",
    "            optimizer=self.optimizer,\n",
    "            data_loader=self.dataloader,\n",
    "            noise_multiplier=0.3,\n",
    "            max_grad_norm=1.5\n",
    "        )\n",
    "        self.grad_tracker = [torch.zeros_like(param) for param in self.model.parameters()]\n",
    "\n",
    "    def train_local(self, global_model, epochs=1, adv_training=True, fkd=True):\n",
    "        self.model.train()\n",
    "        global_params = list(global_model.parameters())\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for data, target in self.dataloader:\n",
    "                data, target = data.to(torch.float32), target.to(torch.long)\n",
    "                if adv_training:\n",
    "                    data = adversarial_attack(self.model, data, target)\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, target)\n",
    "\n",
    "                # FedDyn regularization\n",
    "                fed_dyn_reg = 0.0\n",
    "                for param, g_param, z in zip(self.model.parameters(), global_params, self.grad_tracker):\n",
    "                    fed_dyn_reg += torch.sum(param * (self.mu * (param - g_param.detach()) - z))\n",
    "                loss += fed_dyn_reg\n",
    "\n",
    "                if fkd:\n",
    "                    with torch.no_grad():\n",
    "                        global_output = global_model(data)\n",
    "                    distill_loss = nn.KLDivLoss(reduction='batchmean')(\n",
    "                        F.log_softmax(output, dim=1), F.softmax(global_output, dim=1)\n",
    "                    )\n",
    "                    loss += 0.4 * distill_loss\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "        # Update the clientâ€™s historical gradient (FedDyn)\n",
    "        with torch.no_grad():\n",
    "            for i, (param, g_param) in enumerate(zip(self.model.parameters(), global_params)):\n",
    "                self.grad_tracker[i] -= self.mu * (param.detach() - g_param.detach())\n",
    "\n",
    "        return self.model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab7dec9f-4d8a-4208-b10b-edd6fcfac255",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Server:\n",
    "    def __init__(self, model, num_clients, mu=0.1):\n",
    "        self.global_model = model\n",
    "        self.num_clients = num_clients\n",
    "        self.clients = []\n",
    "        self.mu = mu\n",
    "        self.h_dict = {}  # Drift term for FedDyn\n",
    "\n",
    "    def register_client(self, client):\n",
    "        self.clients.append(client)\n",
    "        self.h_dict[client.client_id] = {k: torch.zeros_like(v) for k, v in self.global_model.state_dict().items()}\n",
    "\n",
    "    def aggregate_weights_feddyn(self, client_updates):\n",
    "        new_global_weights = copy.deepcopy(self.global_model.state_dict())\n",
    "\n",
    "        for key in new_global_weights.keys():\n",
    "            avg_update = torch.stack([update[0][key] - (1 / self.mu) * update[1][key] for update in client_updates])\n",
    "            new_global_weights[key] = avg_update.mean(dim=0)\n",
    "\n",
    "        return new_global_weights\n",
    "\n",
    "    def federated_training(self, rounds=10, epochs=1, adv_training=True, fkd=True):\n",
    "        for r in range(rounds):\n",
    "            # Adaptive client selection based on drift norm (FedDyn)\n",
    "            drift_norms = {client.client_id: torch.norm(torch.cat([v.view(-1) for v in self.h_dict[client.client_id].values()]))\n",
    "                           for client in self.clients}\n",
    "            sorted_clients = sorted(self.clients, key=lambda x: drift_norms[x.client_id], reverse=True)\n",
    "            selected_clients = sorted_clients[:max(1, len(self.clients) // 2)]\n",
    "\n",
    "            print(f\"Round {r+1} | Selected Clients: {[c.client_id for c in selected_clients]}\")\n",
    "\n",
    "            client_updates = []\n",
    "            for client in selected_clients:\n",
    "                state_dict, h_new = client.train_local(self.global_model, self.h_dict[client.client_id], epochs, adv_training, fkd)\n",
    "                client_updates.append((state_dict, self.h_dict[client.client_id]))\n",
    "                self.h_dict[client.client_id] = h_new\n",
    "\n",
    "            # FedDyn Aggregation\n",
    "            new_weights = self.aggregate_weights_feddyn(client_updates)\n",
    "            self.global_model.load_state_dict(new_weights)\n",
    "\n",
    "    def evaluate_model(self, test_loader):\n",
    "        self.global_model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "        total_loss = 0.0\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(torch.float32), target.to(torch.long)\n",
    "                output = self.global_model(data)\n",
    "                loss = criterion(output, target)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                predictions = torch.argmax(output, dim=1)\n",
    "                y_true.extend(target.numpy())\n",
    "                y_pred.extend(predictions.numpy())\n",
    "\n",
    "        # Evaluation Metrics\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred, average='macro')\n",
    "        recall = recall_score(y_true, y_pred, average='macro')\n",
    "        f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "        print(\"Evaluation Metrics:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "\n",
    "        return accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "119ff89c-ddac-462c-b71e-9a0e1f07342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = TabularFeatureEmbedder(input_dim=41, seq_len=41, embed_dim=768)\n",
    "classifier = DistilBERTIntrusionClassifier(hidden_size=768, output_size=2)\n",
    "server_model = CombinedDistilBERTModel(embedder, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87e86483-80e1-4adb-9c92-aa095963c67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\opacus\\privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "server = Server(server_model, num_clients)\n",
    "for i in range(num_clients):\n",
    "    client = Client(i, server_model, client_datasets[i])\n",
    "    server.register_client(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c43983df-544c-4342-81fb-ff17bfa8039c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 completed.\n",
      "Round 2 completed.\n",
      "Round 3 completed.\n",
      "Round 4 completed.\n",
      "Round 5 completed.\n",
      "Round 6 completed.\n",
      "Round 7 completed.\n",
      "Round 8 completed.\n",
      "Round 9 completed.\n",
      "Round 10 completed.\n",
      "Round 11 completed.\n",
      "Round 12 completed.\n",
      "Round 13 completed.\n",
      "Round 14 completed.\n",
      "Round 15 completed.\n",
      "Round 16 completed.\n",
      "Round 17 completed.\n",
      "Round 18 completed.\n",
      "Round 19 completed.\n",
      "Round 20 completed.\n",
      "Round 21 completed.\n",
      "Round 22 completed.\n",
      "Round 23 completed.\n",
      "Round 24 completed.\n",
      "Round 25 completed.\n",
      "Round 26 completed.\n",
      "Round 27 completed.\n",
      "Round 28 completed.\n",
      "Round 29 completed.\n",
      "Round 30 completed.\n",
      "Round 31 completed.\n",
      "Round 32 completed.\n",
      "Round 33 completed.\n",
      "Round 34 completed.\n",
      "Round 35 completed.\n"
     ]
    }
   ],
   "source": [
    "server.federated_training(rounds=35, epochs=5, adv_training=True, dynamic_fed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d241890-b584-4d03-ac3b-0b1a1b1fe07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy:0.9413\n",
      "Recall:0.9316\n",
      "Precision:0.9360\n"
     ]
    }
   ],
   "source": [
    "server.evaluate_model(DataLoader(test_dataset, batch_size=32, shuffle=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9898e84-b269-427e-838f-5dc7c1f5b258",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
